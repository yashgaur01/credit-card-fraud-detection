{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xdExsUYLCdz"
      },
      "outputs": [],
      "source": [
        "pip install pandas numpy scikit-learn matplotlib seaborn imbalanced-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "xsP_j5c1438t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv('/content/drive/MyDrive/FraudTrain.csv')"
      ],
      "metadata": {
        "id": "AXIGaq4LLOzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "W6AO1zFlLpDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.isnull()"
      ],
      "metadata": {
        "id": "AEjqQxxuZQ2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.isnull().sum()"
      ],
      "metadata": {
        "id": "XysAdGhNZZ_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.describe()"
      ],
      "metadata": {
        "id": "4ZrfnB95Zesw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['Index'].value_counts()"
      ],
      "metadata": {
        "id": "ay2UqTYBZid3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.drop(data.columns[:1], axis=1, inplace=True)\n",
        "data.head()"
      ],
      "metadata": {
        "id": "hxeeYyrQZrJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#converting trans_date_trans_time into datetime\n",
        "data['trans_date_trans_time'] = pd.to_datetime(data['trans_date_trans_time'])\n",
        "print(data.dtypes['trans_date_trans_time'])\n",
        "data.head()"
      ],
      "metadata": {
        "id": "iqbqiTStaOPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dtypes of the columns\n",
        "data.dtypes"
      ],
      "metadata": {
        "id": "bJjXDOc5ambH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Number of unique values in the dataset\n",
        "data.nunique()"
      ],
      "metadata": {
        "id": "335pVpIKat2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# deriving additonal columns from 'trans_date_trans_time'\n",
        "#deriving hour\n",
        "data['trans_hour'] = data['trans_date_trans_time'].dt.hour\n",
        "#deriving 'day of the week'\n",
        "data['trans_day_of_week'] = data['trans_date_trans_time'].dt.dayofweek + 1\n",
        "data['trans_day_of_week'] = data['trans_day_of_week'].astype(int)\n",
        "#deriving 'year_month'\n",
        "data['trans_year_month'] = data['trans_date_trans_time'].dt.to_period('M')\n",
        "\n",
        "data.head(10)"
      ],
      "metadata": {
        "id": "a0oOBf9-azkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Getting the Age of a customer from the D.O.B column\n",
        "data['dob'] = pd.to_datetime(data['dob'])\n",
        "data['age'] = ((data['trans_date_trans_time'] - data['dob']).dt.days / 365.25).astype(int)\n",
        "\n",
        "data['age'].head()"
      ],
      "metadata": {
        "id": "rrCMDYKSbV2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "uNtAJVaucRmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "NOChH7UOcLHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AZLlXPxrEI30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#converting data types that should be categorical into \"category\"\n",
        "\n",
        "data['category'] = data['category'].astype('category')\n",
        "data['gender'] = data['gender'].astype('category')\n",
        "data['is_fraud'] = data['is_fraud'].astype('category')\n",
        "\n",
        "data.info()"
      ],
      "metadata": {
        "id": "GeiBd_ebd1xB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "id": "DA6WV6kOeMHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.describe()"
      ],
      "metadata": {
        "id": "rk9E8a1eeQXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check the percentage of fraudulent data points in our dataset\n",
        "100 * data.groupby('is_fraud').size() / len(data)"
      ],
      "metadata": {
        "id": "ZXq8ob9KeWV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#looking at distribution of amount\n",
        "pd.concat(\n",
        "    [data['amt'].describe(percentiles = [0.5,0.95,0.999]).reset_index().rename(columns={'index': 'Row Type', 'amt':'Total Amount Distribution'}),\n",
        "     data.loc[data['is_fraud']==0,['amt']].describe(percentiles = [0.5,0.95,0.999]).reset_index(drop = 1).rename(columns={'amt':'Non-Fraud Amount Distribution'}),\n",
        "     data.loc[data['is_fraud']==1,['amt']].describe(percentiles = [0.5,0.95,0.999]).reset_index(drop = 1).rename(columns={'amt':'Fraud Amount Distribution'})], axis=1)"
      ],
      "metadata": {
        "id": "A8KnigH5eiLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting the above distributions\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "fig = plt.subplots(figsize=(15,10))\n",
        "\n",
        "plots = []\n",
        "#plotting the amt feature\n",
        "\n",
        "#distribution plots\n",
        "plots.append(sns.histplot(data[data.amt <= 1500].amt, bins=50, ax=plt.subplot(234)))\n",
        "plots.append(sns.histplot(data[(data.is_fraud==0) & (data.amt<=1500)].amt, bins=50, ax=plt.subplot(235)))\n",
        "plots.append(sns.histplot(data[(data.is_fraud==1) & (data.amt<=1500)].amt, bins=50, ax=plt.subplot(236)))\n",
        "\n",
        "#setting titles\n",
        "plots[0].set_title('Overall Amount Distribution')\n",
        "plots[1].set_title('Non Fraud Amount Distribution')\n",
        "plots[2].set_title('Fraud Amount Distribution')\n",
        "\n",
        "#setting x labels\n",
        "plots[0].set_xlabel('Transaction Amount')\n",
        "plots[1].set_xlabel('Transaction Amount')\n",
        "plots[2].set_xlabel('Transaction Amount')\n",
        "\n",
        "#setting y label\n",
        "plots[0].set_ylabel('Number of transactions')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WiQxFlpLG-d4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#year_month vs number of transactions\n",
        "df_timeline01 = data.groupby(data['trans_year_month'])[['trans_num','cc_num']].nunique().reset_index()\n",
        "df_timeline01.columns = ['year_month','num_of_transactions','customers']\n",
        "df_timeline01"
      ],
      "metadata": {
        "id": "qsmeByNUHavC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_transactions = data[data['is_fraud']==1]\n",
        "\n",
        "df_timeline02 = data_transactions.groupby(data_transactions['trans_year_month'])[['trans_num','cc_num']].nunique().reset_index()\n",
        "df_timeline02.columns = ['year_month','num_of_fraud_transactions','fraud_customers']\n",
        "df_timeline02"
      ],
      "metadata": {
        "id": "9BZOiTwNIsYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gender_dist = data['gender'].value_counts()\n",
        "print(gender_dist)"
      ],
      "metadata": {
        "id": "e0a3c9ebI72C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gender_fraud_dist = data.groupby('gender')['is_fraud'].value_counts(normalize=True).unstack()\n",
        "print(gender_fraud_dist)\n"
      ],
      "metadata": {
        "id": "WKpO-pgCLVpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the Distribution\n",
        "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# Gender distribution\n",
        "sns.countplot(x='gender', data=data, ax=axs[0])\n",
        "axs[0].set_title('Gender Distribution')\n",
        "axs[0].set_xlabel('Gender')\n",
        "axs[0].set_ylabel('Count')\n",
        "\n",
        "# Gender-fraud distribution\n",
        "gender_fraud_dist.plot(kind='bar', stacked=True, ax=axs[1])\n",
        "axs[1].set_title('Gender-Fraud Distribution')\n",
        "axs[1].set_xlabel('Gender')\n",
        "axs[1].set_ylabel('Proportion')\n",
        "axs[1].legend(title='Is Fraud')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kLts59uRPmYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bins = [12, 19, 32, 42, 50,62 , float('inf')]\n",
        "custom_labels = ['Teenagers', 'Young Adults', 'Adults', 'Middle-aged', 'Seniors', 'Retired']\n",
        "\n",
        "# Apply the binning to create a new 'age_category' column\n",
        "data['age_category'] = pd.cut(data['age'], bins=bins, labels=custom_labels, right=False)\n",
        "\n",
        "# Display the result\n",
        "print(data[['age', 'age_category']].tail())"
      ],
      "metadata": {
        "id": "VhD4m6vEPsfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "\n",
        "sns.countplot(x='age_category', data=data, ax=ax)\n",
        "ax.set_title('Age Category Distribution')\n",
        "ax.set_xlabel('Age Category')\n",
        "ax.set_ylabel('Count')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "QTMfer6IQ0-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group the data and calculate counts\n",
        "grouped_data = data.groupby(['age_category', 'is_fraud'])['age'].count().unstack()\n",
        "# Plot the bar chart\n",
        "grouped_data.plot(kind='bar', figsize=(10, 6))\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Age Category')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Distribution of Age Categories by Fraud Status')\n",
        "plt.xticks(rotation=0)\n",
        "plt.legend(title='Is Fraud')\n",
        "\n",
        "# Display the plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "D2pgklZoQ5Ny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plotting the count of individuals in each age category for both fraudulent and non-fraudulent transactions\n",
        "ax = sns.countplot(x='age_category', hue='is_fraud', data=data, order=custom_labels, palette='viridis')\n",
        "\n",
        "# Adding labels and title\n",
        "plt.xlabel('Age Category')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Distribution of Age Categories for Fraudulent and Non-Fraudulent Transactions')\n",
        "\n",
        "\n",
        "\n",
        "# Adding the ratio of fraudulent transactions for each category\n",
        "for category in custom_labels:\n",
        "    total_count = data[data['age_category'] == category].shape[0]\n",
        "    fraud_count = data[(data['age_category'] == category) & (data['is_fraud'] == 1)].shape[0]\n",
        "    ratio = fraud_count / total_count if total_count > 0 else 0\n",
        "    ax.text(custom_labels.index(category), total_count, f'Fraud Ratio: {ratio:.2%}',\n",
        "            ha='center', va='bottom', fontsize=10, color='red')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "k0H66kluQ8ho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.columns"
      ],
      "metadata": {
        "id": "HV4kI7pGw3qn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# One-hot encoding categorical variables\n",
        "data = pd.get_dummies(data, columns=['category', 'gender'])"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "siYgtiWFUjxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop columns that won't be used for modeling\n",
        "data.drop(columns=['first', 'last', 'street', 'city', 'state', 'zip', 'lat', 'long', 'job', 'unix_time', 'merch_lat', 'merch_long'], inplace=True)"
      ],
      "metadata": {
        "id": "UJ2rHJSXUmFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting data into features and target\n",
        "X = data.drop(columns=['is_fraud'])\n",
        "y = data['is_fraud'].astype('int')"
      ],
      "metadata": {
        "id": "i_a76UAzlUCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)"
      ],
      "metadata": {
        "id": "f3Uy220Wmku5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert datetime features to numerical representation\n",
        "# Extract numerical features from datetime\n",
        "# Example for 'trans_date_trans_time' column:\n",
        "# Check if the column exists before processing\n",
        "if 'trans_date_trans_time' in X_train.columns:\n",
        "    X_train['trans_date_trans_time_numeric'] = X_train['trans_date_trans_time'].astype('int64') // 10**9  # Convert to Unix timestamp\n",
        "    X_test['trans_date_trans_time_numeric'] = X_test['trans_date_trans_time'].astype('int64') // 10**9\n",
        "\n",
        "    # Drop original datetime column\n",
        "    X_train.drop(columns=['trans_date_trans_time'], inplace=True)\n",
        "    X_test.drop(columns=['trans_date_trans_time'], inplace=True)\n",
        "else:\n",
        "    print(\"Column 'trans_date_trans_time' not found. Skipping this step.\")\n",
        "\n",
        "# Identify and drop non-numerical columns before scaling\n",
        "non_numeric_columns = X_train.select_dtypes(exclude=['number']).columns\n",
        "X_train = X_train.drop(columns=non_numeric_columns)\n",
        "X_test = X_test.drop(columns=non_numeric_columns)\n",
        "\n",
        "# Now scale the numerical features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "_QfxmEaGmksw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Applying SMOTE to balance the classes in the training set\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "yqk3wXOYaORB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression with balanced classes\n",
        "log_reg = LogisticRegression(class_weight='balanced')\n",
        "log_reg.fit(X_train_resampled, y_train_resampled)\n",
        "y_pred_log_reg = log_reg.predict(X_test)\n",
        "\n",
        "print(\"Logistic Regression\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_log_reg)}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred_log_reg)}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred_log_reg)}\")\n",
        "print(f\"F1 Score: {f1_score(y_test, y_pred_log_reg)}\")"
      ],
      "metadata": {
        "id": "3gZ9l4_7awNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "y_pred_log_reg = log_reg.predict(X_test)\n",
        "\n",
        "# Generate the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred_log_reg)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Fraud', 'Fraud'], yticklabels=['Not Fraud', 'Fraud'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix for Logistic Regression')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "4XW-YY4CjiIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decision Tree Classiifer with balanced classes\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "dt_clf = DecisionTreeClassifier(class_weight='balanced')\n",
        "dt_clf.fit(X_train_resampled, y_train_resampled)\n",
        "y_pred_dt_clf = dt_clf.predict(X_test)\n",
        "\n",
        "print(\"Decision Tree Classifier\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_dt_clf)}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred_dt_clf)}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred_dt_clf)}\")\n",
        "print(f\"F1 Score: {f1_score(y_test, y_pred_dt_clf)}\")"
      ],
      "metadata": {
        "id": "sIC7kA1UmkqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y_pred_dt_clf = dt_clf.predict(X_test)\n",
        "\n",
        "# Generate the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred_dt_clf)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Fraud', 'Fraud'], yticklabels=['Not Fraud', 'Fraud'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix for Decision Tree Classifier')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "a0ZUTAfxj5kx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest Classifier with balanced classes\n",
        "rf_clf = RandomForestClassifier(class_weight='balanced')\n",
        "rf_clf.fit(X_train_resampled, y_train_resampled)\n",
        "y_pred_rf_clf = rf_clf.predict(X_test)\n",
        "\n",
        "print(\"Random Forest Classifier\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_rf_clf)}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred_rf_clf)}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred_rf_clf)}\")\n",
        "print(f\"F1 Score: {f1_score(y_test, y_pred_rf_clf)}\")"
      ],
      "metadata": {
        "id": "hwqCKgplmklm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y_pred_rf_clf = rf_clf.predict(X_test)\n",
        "\n",
        "# Generate the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred_rf_clf)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Fraud', 'Fraud'], yticklabels=['Not Fraud', 'Fraud'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix for Random Forest Classifier')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "w3jREJkSkOAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# XGBoost Classifier with balanced classes\n",
        "xgb_clf = XGBClassifier(scale_pos_weight=(len(y_train) - sum(y_train)) / sum(y_train), use_label_encoder=False, eval_metric='mlogloss')\n",
        "xgb_clf.fit(X_train_resampled, y_train_resampled)\n",
        "y_pred_xgb = xgb_clf.predict(X_test)\n",
        "\n",
        "print(\"XGBoost Classifier\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_xgb)}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred_xgb)}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred_xgb)}\")\n",
        "print(f\"F1 Score: {f1_score(y_test, y_pred_xgb)}\")"
      ],
      "metadata": {
        "id": "rwkPYDMBmZ0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y_pred_xgb = xgb_clf.predict(X_test)\n",
        "\n",
        "# Generate the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred_xgb)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Fraud', 'Fraud'], yticklabels=['Not Fraud', 'Fraud'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix for XGBoost Classifier')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "UEjG4HC_U4Ja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "# SGDClassifier with hinge loss, which acts as a linear SVM\n",
        "sgd_clf = SGDClassifier(loss='hinge', class_weight='balanced', random_state=42, max_iter=1000)\n",
        "sgd_clf.fit(X_train_resampled, y_train_resampled)\n",
        "y_pred_sgd = sgd_clf.predict(X_test)\n",
        "\n",
        "print(\"SGDClassifier (Linear SVM)\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_sgd)}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred_sgd)}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred_sgd)}\")\n",
        "print(f\"F1 Score: {f1_score(y_test, y_pred_sgd)}\")\n"
      ],
      "metadata": {
        "id": "6NwnOW2_UzVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y_pred_sgd_clf = sgd_clf.predict(X_test)\n",
        "\n",
        "# Generate the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred_sgd)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Fraud', 'Fraud'], yticklabels=['Not Fraud', 'Fraud'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix for Stochastic Gradient Descent SVM Classifier')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "hdOxP6LgVPR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import SGDClassifier\n"
      ],
      "metadata": {
        "id": "epomo0k8XpVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Define a smaller set of base models for quicker testing\n",
        "estimators = [\n",
        "    ('rf', RandomForestClassifier(class_weight='balanced', random_state=42)),\n",
        "    ('xgb', XGBClassifier(scale_pos_weight=(len(y_train) - sum(y_train)) / sum(y_train), random_state=42)),\n",
        "    ('sgd', SGDClassifier(loss='log_loss', class_weight='balanced', random_state=42))  # Fixed loss parameter\n",
        "]\n",
        "\n",
        "# Create the stacking classifier\n",
        "stacking_clf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\n",
        "\n",
        "# Time the fitting process\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "# Fit the stacking model\n",
        "stacking_clf.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"Fitting time: {end_time - start_time} seconds\")\n",
        "\n",
        "# Predict and evaluate the stacking model\n",
        "y_pred_stacking = stacking_clf.predict(X_test)\n",
        "\n",
        "print(\"Stacking Classifier\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_stacking)}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred_stacking)}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred_stacking)}\")\n",
        "print(f\"F1 Score: {f1_score(y_test, y_pred_stacking)}\")\n"
      ],
      "metadata": {
        "id": "9MeL_dp7YXJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install shap\n"
      ],
      "metadata": {
        "id": "W2GccM9_oOgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lime\n"
      ],
      "metadata": {
        "id": "5ZWOFQ5DoYsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "id": "j3RcrWmQoej7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier, VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import shap\n",
        "from lime.lime_tabular import LimeTabularExplainer\n",
        "import optuna\n",
        "\n",
        "\n",
        "# Sample feature engineering\n",
        "data['amt_age_interaction'] = data['amt'] * data['age']\n",
        "\n",
        "# Apply SMOTE for balancing the classes\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Feature Importance-based Feature Selection\n",
        "rf_clf = RandomForestClassifier()\n",
        "rf_clf.fit(X_train_resampled, y_train_resampled)\n",
        "importances = rf_clf.feature_importances_\n",
        "indices = np.argsort(importances)[::-1]\n",
        "top_features = X.columns[indices[:10]]\n",
        "X_train_resampled = X_train_resampled[:, indices[:10]]\n",
        "X_test = X_test[:, indices[:10]]\n",
        "\n",
        "# Define base models\n",
        "estimators = [\n",
        "    ('rf', RandomForestClassifier()),\n",
        "    ('xgb', XGBClassifier()),\n",
        "    ('svc', SVC())\n",
        "]\n",
        "\n",
        "# Create the stacking classifier\n",
        "stacking_clf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\n",
        "\n",
        "# Fit the stacking model\n",
        "stacking_clf.fit(X_train_resampled, y_train_resampled)\n",
        "y_pred_stack = stacking_clf.predict(X_test)\n",
        "\n",
        "# Evaluate the stacking model\n",
        "print(\"Stacking Classifier\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_stack)}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred_stack)}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred_stack)}\")\n",
        "print(f\"F1 Score: {f1_score(y_test, y_pred_stack)}\")\n",
        "print(\"Classification Report\")\n",
        "print(classification_report(y_test, y_pred_stack))\n",
        "\n",
        "# Model Interpretability with SHAP\n",
        "explainer = shap.Explainer(stacking_clf, X_train_resampled)\n",
        "shap_values = explainer(X_test)\n",
        "shap.summary_plot(shap_values, X_test)\n",
        "\n",
        "# Model Interpretability with LIME\n",
        "explainer = LimeTabularExplainer(X_train_resampled, feature_names=top_features, class_names=['Not Fraud', 'Fraud'], mode='classification')\n",
        "idx = 0  # Example index to explain\n",
        "exp = explainer.explain_instance(X_test[idx], stacking_clf.predict_proba)\n",
        "exp.show_in_notebook()\n",
        "\n",
        "# Hyperparameter Tuning with Optuna\n",
        "def objective(trial):\n",
        "    param = {\n",
        "        'max_depth': trial.suggest_int('max_depth', 2, 10),\n",
        "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1e-1),\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
        "        'objective': 'binary:logistic'\n",
        "    }\n",
        "    model = XGBClassifier(**param)\n",
        "    model.fit(X_train_resampled, y_train_resampled)\n",
        "    y_pred = model.predict(X_test)\n",
        "    return 1 - f1_score(y_test, y_pred)\n",
        "\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=50)\n",
        "\n",
        "print(f\"Best parameters: {study.best_params}\")\n",
        "\n",
        "# Additional ensemble techniques: Voting Classifier\n",
        "voting_clf = VotingClassifier(estimators=[\n",
        "    ('lr', LogisticRegression()),\n",
        "    ('rf', RandomForestClassifier()),\n",
        "    ('xgb', XGBClassifier())\n",
        "], voting='soft')\n",
        "\n",
        "voting_clf.fit(X_train_resampled, y_train_resampled)\n",
        "y_pred_voting = voting_clf.predict(X_test)\n",
        "\n",
        "print(\"Voting Classifier\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_voting)}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred_voting)}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred_voting)}\")\n",
        "print(f\"F1 Score: {f1_score(y_test, y_pred_voting)}\")\n",
        "print(\"Classification Report\")\n",
        "print(classification_report(y_test, y_pred_voting))\n"
      ],
      "metadata": {
        "id": "9pcW7SbGoDrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.svm import LinearSVC\n",
        "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# # Linear SVM Classifier with balanced classes\n",
        "# linear_svm_clf = LinearSVC(class_weight='balanced', max_iter=10000)\n",
        "# linear_svm_clf.fit(X_train_resampled, y_train_resampled)\n",
        "# y_pred_linear_svm = linear_svm_clf.predict(X_test)\n",
        "\n",
        "# print(\"Linear SVM Classifier\")\n",
        "# print(f\"Accuracy: {accuracy_score(y_test, y_pred_linear_svm)}\")\n",
        "# print(f\"Precision: {precision_score(y_test, y_pred_linear_svm)}\")\n",
        "# print(f\"Recall: {recall_score(y_test, y_pred_linear_svm)}\")\n",
        "# print(f\"F1 Score: {f1_score(y_test, y_pred_linear_svm)}\")\n"
      ],
      "metadata": {
        "id": "GpVmz3L4rbZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Support Vector Machine Classifier with balanced classes\n",
        "# svm_clf = SVC(class_weight='balanced')\n",
        "# svm_clf.fit(X_train_resampled, y_train_resampled)\n",
        "# y_pred_svm = svm_clf.predict(X_test)\n",
        "\n",
        "# print(\"SVM Classifier\")\n",
        "# print(f\"Accuracy: {accuracy_score(y_test, y_pred_svm)}\")\n",
        "# print(f\"Precision: {precision_score(y_test, y_pred_svm)}\")\n",
        "# print(f\"Recall: {recall_score(y_test, y_pred_svm)}\")\n",
        "# print(f\"F1 Score: {f1_score(y_test, y_pred_svm)}\")"
      ],
      "metadata": {
        "id": "TjMkWLwzt95L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qUjOe0RA0Mu3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}